import os
import pandas as pd
import math

def convert_then_split_parquet_safe(file_path: str, max_size_kb: int = 100, sample_lines: int = 500, safety_margin: float = 0.9):
    """
    Convertit un CSV en Parquet et dÃ©coupe en chunks sous max_size_kb.
    MÃªme si les chunks tests dÃ©passent dÃ©jÃ  la limite, ajuste automatiquement.
    """

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"{file_path} introuvable.")

    parquet_file = file_path.replace(".csv", ".parquet")
    df = pd.read_csv(file_path)
    df.to_parquet(parquet_file, index=False)
    print(f"ğŸ”„ Conversion du CSV vers Parquet : {parquet_file}")
    print(f"âœ… Fichier Parquet crÃ©Ã© : {parquet_file}")

    max_bytes = max_size_kb * 1024
    file_size_bytes = os.path.getsize(parquet_file)
    print(f"ğŸ“¦ Taille du fichier Parquet : {file_size_bytes / 1024:.2f} KB")

    # Si le fichier est dÃ©jÃ  infÃ©rieur Ã  la limite
    if file_size_bytes <= max_bytes:
        print(f"â„¹ï¸ {parquet_file} est infÃ©rieur Ã  {max_size_kb} KB. Pas de dÃ©coupage nÃ©cessaire.")
        return [parquet_file]

    # --- Estimation initiale ---
    sample = df.head(sample_lines)
    temp_sample = "sample_temp.parquet"
    sample.to_parquet(temp_sample, index=False)
    avg_bytes_per_line = os.path.getsize(temp_sample) / len(sample)
    os.remove(temp_sample)

    rows_per_chunk = max(1, int(max_bytes * safety_margin / avg_bytes_per_line))
    print(f"ğŸ“ˆ Estimation initiale : {rows_per_chunk} lignes par chunk")

    # --- Fonction pour dÃ©couper et mesurer ---
    def split_and_measure(rows_per_chunk):
        parts = []
        total_rows = len(df)
        for i, start in enumerate(range(0, total_rows, rows_per_chunk), 1):
            chunk = df.iloc[start:start + rows_per_chunk]
            out = f"data_part_{i:02d}.parquet"
            chunk.to_parquet(out, index=False)
            parts.append((out, os.path.getsize(out)))
        return parts

    # Boucle d'ajustement automatique
    while True:
        parts = split_and_measure(rows_per_chunk)
        max_part_size = max(size for _, size in parts)
        if max_part_size <= max_bytes:
            break
        # RÃ©duction progressive si un chunk dÃ©passe la limite
        reduction_ratio = max_bytes / max_part_size * safety_margin
        new_rows_per_chunk = max(1, int(rows_per_chunk * reduction_ratio))
        if new_rows_per_chunk == rows_per_chunk:
            # On ne peut plus rÃ©duire, sortie pour Ã©viter boucle infinie
            print("âš ï¸ Impossible de respecter la limite exacte, fichier final lÃ©gÃ¨rement supÃ©rieur.")
            break
        rows_per_chunk = new_rows_per_chunk
        # Supprimer fichiers gÃ©nÃ©rÃ©s prÃ©cÃ©demment
        for f, _ in parts:
            os.remove(f)

    # Affichage final
    for f, size in parts:
        size_kb = size / 1024
        print(f"âœ… {f} crÃ©Ã© ({size_kb:.2f} KB)")
        if size > max_bytes:
            print(f"âš ï¸ {f} dÃ©passe la limite de {max_size_kb} KB")

    print(f"ğŸ‰ DÃ©coupage terminÃ© : {len(parts)} fichiers gÃ©nÃ©rÃ©s")
    return [f for f, _ in parts]

# Exemple d'utilisation
if __name__ == "__main__":
    convert_then_split_parquet_safe("../donnees/bd_algo_2_version2.csv", max_size_kb=100, sample_lines=1406)
